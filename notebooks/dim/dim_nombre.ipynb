{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d355e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b46875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Nombres guardados en min√∫sculas y IDs generados correctamente.\n",
      "üìÅ Archivo listo en:\n",
      "H:\\git\\proyecto grupal 2\\Yelp-Gmaps-Proyecto-DS\\data\\processed\\dim\\dim_nombre_restaurante.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrgsi\\AppData\\Local\\Temp\\ipykernel_13432\\1805473131.py:24: DtypeWarning: Columns (3,4,5,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ruta_csv)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import unicodedata\n",
    "\n",
    "# üéØ Funci√≥n para generar ID hash base36 de 6 caracteres\n",
    "def generar_id_hash(texto):\n",
    "    h = hashlib.md5(texto.encode('utf-8')).hexdigest()\n",
    "    base10 = int(h, 16)\n",
    "    base36 = ''\n",
    "    chars = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    while base10 > 0:\n",
    "        base10, i = divmod(base10, 36)\n",
    "        base36 = chars[i] + base36\n",
    "    return base36[:6].upper().zfill(6)\n",
    "\n",
    "# üîß Normaliza (quita tildes, espacios y convierte a min√∫sculas)\n",
    "def normalizar(texto):\n",
    "    texto = str(texto).lower().strip()\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    return texto\n",
    "\n",
    "# üóÇ Ruta del CSV\n",
    "ruta_csv = r'H:\\git\\proyecto grupal 2\\Yelp-Gmaps-Proyecto-DS\\data\\raw\\departamento salubridad NY\\rows2.csv'\n",
    "df = pd.read_csv(ruta_csv)\n",
    "\n",
    "# üîç Extraer nombres √∫nicos\n",
    "nombres_originales = df['nombre'].dropna().drop_duplicates().sort_values().reset_index(drop=True)\n",
    "\n",
    "# üßº Guardar los nombres normalizados (min√∫sculas + sin acento)\n",
    "nombres_minusculas = nombres_originales.apply(lambda x: normalizar(x))\n",
    "\n",
    "# üîê Generar los IDs hash desde los nombres normalizados\n",
    "ids_hash = nombres_minusculas.apply(generar_id_hash)\n",
    "\n",
    "# üß± Construir la tabla dimensi√≥n con los nombres ya en min√∫sculas\n",
    "dim_nombres = pd.DataFrame({\n",
    "    'id_nombre': ids_hash,\n",
    "    'nombre': nombres_minusculas  # AQU√ç se guardan ya en min√∫sculas\n",
    "})\n",
    "\n",
    "# üíæ Guardar\n",
    "ruta_salida = r'H:\\git\\proyecto grupal 2\\Yelp-Gmaps-Proyecto-DS\\data\\processed\\dim\\dim_nombre_restaurante.csv'\n",
    "dim_nombres.to_csv(ruta_salida, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"‚úÖ Nombres guardados en min√∫sculas y IDs generados correctamente.\\nüìÅ Archivo listo en:\\n{ruta_salida}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6f535f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 12:34:00 [INFO] __main__.main_limpieza_dim_nombres: ================================================\n",
      "2025-04-19 12:34:00 [INFO] __main__.main_limpieza_dim_nombres: === INICIO Limpieza Dimensi√≥n Nombres Restaurante ===\n",
      "2025-04-19 12:34:00 [INFO] __main__.main_limpieza_dim_nombres: ================================================\n",
      "2025-04-19 12:34:00 [INFO] __main__.main_limpieza_dim_nombres: Paso 1: Cargando dimensi√≥n desde: H:\\git\\proyecto grupal 2\\Yelp-Gmaps-Proyecto-DS\\data\\processed\\dim\\dim_nombre_restaurante_limpia.csv\n",
      "2025-04-19 12:34:00 [INFO] __main__.main_limpieza_dim_nombres: Dimensi√≥n cargada: 49322 filas, 3 columnas.\n",
      "2025-04-19 12:34:00 [INFO] __main__.main_limpieza_dim_nombres: Paso 2: Realizando limpieza b√°sica inicial...\n",
      "2025-04-19 12:34:01 [INFO] __main__.main_limpieza_dim_nombres: Columna 'permiso_temporal' asegurada como tipo Boolean (soporta nulos si los hubiera).\n",
      "2025-04-19 12:34:01 [INFO] __main__.main_limpieza_dim_nombres: Filtrando filas con IDs o nombres inv√°lidos/vac√≠os/ruidosos...\n",
      "2025-04-19 12:34:01 [INFO] __main__.main_limpieza_dim_nombres: Eliminadas 1 filas con IDs/nombres inv√°lidos/vac√≠os/ruidosos.\n",
      "2025-04-19 12:34:01 [INFO] __main__.main_limpieza_dim_nombres: Eliminando duplicados por 'nombre' (insensible a may√∫s/min√∫s, sin espacios extra)...\n",
      "2025-04-19 12:34:01 [INFO] __main__.main_limpieza_dim_nombres: Eliminados 5806 duplicados por 'nombre' (limpio, case-insensitive).\n",
      "2025-04-19 12:34:01 [INFO] __main__.main_limpieza_dim_nombres: Paso 3: Normalizando nombres y generando reporte de duplicados...\n",
      "2025-04-19 12:34:01 [INFO] __main__.identificar_y_reportar_duplicados: Iniciando normalizaci√≥n y b√∫squeda de posibles duplicados...\n",
      "2025-04-19 12:34:01 [INFO] __main__.identificar_y_reportar_duplicados: Aplicando funci√≥n de normalizaci√≥n a 'nombre'...\n",
      "2025-04-19 12:34:02 [WARNING] __main__.identificar_y_reportar_duplicados: 20 nombres resultaron nulos/vac√≠os tras normalizar (ser√°n ignorados en la detecci√≥n).\n",
      "2025-04-19 12:34:05 [WARNING] __main__.identificar_y_reportar_duplicados: ¬°Ojo! Encontramos 3536 grupos de nombres que parecen ser el mismo restaurante pero tienen IDs diferentes.\n",
      "2025-04-19 12:34:05 [INFO] __main__.identificar_y_reportar_duplicados: Reporte de duplicados potenciales guardado en: H:\\git\\proyecto grupal 2\\Yelp-Gmaps-Proyecto-DS\\data\\processed\\posibles_duplicados_nombres.csv\n",
      "2025-04-19 12:34:05 [INFO] __main__.identificar_y_reportar_duplicados: Normalizaci√≥n e identificaci√≥n de duplicados completada.\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: Paso 4: Ordenando la dimensi√≥n alfab√©ticamente por 'nombre'...\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: Dimensi√≥n ordenada.\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: Paso 5: Preparando DataFrame final para guardar...\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: Columna 'nombre_normalizado' eliminada para el archivo final.\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: Columnas finales seleccionadas para guardar: ['id_nombre', 'nombre', 'permiso_temporal']\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: Paso 6: Guardando la dimensi√≥n final...\n",
      "2025-04-19 12:34:05 [INFO] __main__.guardar_dimension: Guardando dimensi√≥n final (43515 filas) en: H:\\git\\proyecto grupal 2\\Yelp-Gmaps-Proyecto-DS\\data\\processed\\dim\\dim_nombre_restaurante_limpia.csv\n",
      "2025-04-19 12:34:05 [INFO] __main__.guardar_dimension: ¬°Dimensi√≥n guardada exitosamente!\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: ================================================\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: === Limpieza Dimensi√≥n Nombres Completada ===\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: === Reporte de posibles duplicados generado en: H:\\git\\proyecto grupal 2\\Yelp-Gmaps-Proyecto-DS\\data\\processed\\posibles_duplicados_nombres.csv ===\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: === ¬°Revisa ese archivo para decidir c√≥mo unificar los IDs! ===\n",
      "2025-04-19 12:34:05 [INFO] __main__.main_limpieza_dim_nombres: ================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Script DEDICADO a la limpieza, normalizaci√≥n y an√°lisis de duplicados\n",
    "# de nuestra tabla de dimensi√≥n de nombres de restaurantes. ¬°Vamos a dejarla impecable!\n",
    "\n",
    "# --- Importaciones Esenciales ---\n",
    "# Cargamos nuestras herramientas principales\n",
    "import pandas as pd           # Nuestro caballo de batalla para manejar datos\n",
    "import numpy as np            # Para operaciones num√©ricas y el √∫til NaN (Not a Number)\n",
    "import re                     # Expresiones regulares, ¬°geniales para buscar patrones en texto!\n",
    "import unicodedata            # Para manejar caracteres especiales y acentos\n",
    "import logging                # Nuestro diario de a bordo para saber qu√© hace el script\n",
    "from pathlib import Path      # Para manejar rutas de archivo de forma moderna y compatible\n",
    "import sys                    # Para poder detener el script si algo va muy mal\n",
    "from io import StringIO       # Para capturar info() en logs (opcional pero √∫til)\n",
    "\n",
    "# --- Configuraci√≥n de Logging (Nuestro Diario de a Bordo) ---\n",
    "# Configuramos c√≥mo queremos ver los mensajes mientras corre el script.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, # Mostraremos mensajes informativos, de advertencia y errores.\n",
    "    format='%(asctime)s [%(levelname)s] %(name)s.%(funcName)s: %(message)s', # Formato detallado\n",
    "    datefmt='%Y-%m-%d %H:%M:%S' # Formato de fecha y hora local\n",
    ")\n",
    "log = logging.getLogger(__name__) # Creamos un logger espec√≠fico para este script.\n",
    "\n",
    "# --- Definici√≥n de Rutas (El Mapa del Tesoro) ---\n",
    "# Centralizamos todas las rutas aqu√≠ para encontrarlas f√°cil.\n",
    "try:\n",
    "    # 1. RUTA_BASE: La carpeta principal de nuestro proyecto.\n",
    "    #    *** ¬°OJO! Revisa que esta ruta sea correcta en TU computadora. ***\n",
    "    RUTA_BASE = Path(r\"H:\\git\\proyecto grupal 2\\Yelp-Gmaps-Proyecto-DS\").resolve()\n",
    "\n",
    "    # 2. Subcarpetas est√°ndar (las creamos si no existen m√°s abajo).\n",
    "    RUTA_PROCESSED = RUTA_BASE / \"data\" / \"processed\" # Donde van nuestros datos limpios.\n",
    "    RUTA_DIM = RUTA_PROCESSED / \"dim\"                # Donde van nuestras tablas de dimensi√≥n.\n",
    "\n",
    "    # 3. Ruta a la Dimensi√≥n de Nombres (¬°Nuestro objetivo!)\n",
    "    #    Este es el archivo que vamos a leer y LUEGO sobreescribir con la versi√≥n limpia.\n",
    "    #    *** ¬°Aseg√∫rate de que el nombre del archivo sea el correcto! ***\n",
    "    RUTA_DIM_NOMBRES = (RUTA_DIM / \"dim_nombre_restaurante_limpia.csv\").resolve()\n",
    "    # Si se llama diferente, aj√∫stalo aqu√≠:\n",
    "    # RUTA_DIM_NOMBRES = (RUTA_DIM / \"dim_nombre_restaurante.csv\").resolve()\n",
    "\n",
    "    # 4. Ruta para el Reporte de Duplicados que generaremos.\n",
    "    RUTA_REPORTE_DUPLICADOS = (RUTA_PROCESSED / \"posibles_duplicados_nombres.csv\").resolve()\n",
    "\n",
    "    # 5. Crear las carpetas si no existen (¬°as√≠ no falla el guardado!)\n",
    "    RUTA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "    RUTA_DIM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"¬°Error cr√≠tico! No pudimos definir o resolver las rutas. Revisa la RUTA_BASE: {e}\")\n",
    "    sys.exit(1) # Detenemos el script si no podemos ni definir las rutas.\n",
    "\n",
    "# --- Funciones de Limpieza y Normalizaci√≥n ---\n",
    "\n",
    "def normalizar_nombre_restaurante(nombre: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Nuestra funci√≥n \"pulidora\" de nombres. Intenta estandarizar al m√°ximo\n",
    "    para encontrar duplicados que se escriben diferente.\n",
    "\n",
    "    Args:\n",
    "        nombre (str): El nombre original del restaurante.\n",
    "\n",
    "    Returns:\n",
    "        str or None: El nombre normalizado, o None si no es v√°lido.\n",
    "    \"\"\"\n",
    "    # Si no es texto o es nulo, no podemos hacer nada.\n",
    "    if pd.isna(nombre) or not isinstance(nombre, str):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # 1. A min√∫sculas y quitamos espacios al inicio/final.\n",
    "        nombre_norm = nombre.lower().strip()\n",
    "\n",
    "        # 2. Quitar comillas comunes al inicio/final (simples, dobles, triples).\n",
    "        nombre_norm = re.sub(r'^[\"\\']{1,3}|[\"\\']{1,3}$', '', nombre_norm).strip()\n",
    "\n",
    "        # 3. Quitar caracteres especiales comunes al inicio (ej. $1 Pizza -> 1 Pizza).\n",
    "        #    Hay que tener cuidado para no quitar algo importante.\n",
    "        nombre_norm = re.sub(r'^[#$*%\\+\\?¬ø!¬°\\-_]+', '', nombre_norm).strip()\n",
    "\n",
    "        # 4. Normalizaci√≥n Unicode: Adi√≥s acentos y caracteres \"raros\".\n",
    "        nombre_norm = unicodedata.normalize('NFKD', nombre_norm).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        # 5. Eliminar sufijos comunes de negocios y tipos de lugar.\n",
    "        #    Usamos \\b para asegurar que sean palabras completas (ej. no quitar \"inc\" de \"zinc\").\n",
    "        sufijos_comunes = [\n",
    "            r'\\bllc\\b', r'\\bltd\\b', r'\\binc\\b', r'\\bcorp\\b', r'\\bcorporation\\b',\n",
    "            r'\\brestaurant\\b', r'\\bcafe\\b', r'\\bpizzeria\\b', r'\\bgrill\\b',\n",
    "            r'\\bbar\\b', r'\\bdeli\\b', r'\\bexpress\\b', r'\\bkitchen\\b',\n",
    "            r'\\bthe\\b' # \"the\" como palabra suelta.\n",
    "        ]\n",
    "        for sufijo in sufijos_comunes:\n",
    "            # flags=re.IGNORECASE para que no importe si es LLC o llc.\n",
    "            nombre_norm = re.sub(sufijo, '', nombre_norm, flags=re.IGNORECASE)\n",
    "\n",
    "        # 6. Manejo de Puntuaci√≥n: Quitamos casi todo, pero intentamos mantener ap√≥strofes internos (como en Joe's).\n",
    "        nombre_norm = nombre_norm.replace(\"'\", \"__APOSTROPHE__\") # Reemplazo temporal.\n",
    "        # Quitamos todo lo que NO sea letra, n√∫mero, espacio o nuestro placeholder.\n",
    "        nombre_norm = re.sub(r'[^\\w\\s__APOSTROPHE__]+', '', nombre_norm.replace('_',' '))\n",
    "        nombre_norm = nombre_norm.replace(\"__APOSTROPHE__\", \"'\") # Lo restauramos.\n",
    "\n",
    "        # 7. Normalizar espacios: M√∫ltiples espacios se vuelven uno, y quitamos inicio/final.\n",
    "        nombre_norm = re.sub(r'\\s+', ' ', nombre_norm).strip()\n",
    "\n",
    "        # 8. Filtrar nombres que claramente no son v√°lidos o son ruido.\n",
    "        placeholders_ruido = {'name', '#name?', '', 'test'}\n",
    "        # Si el nombre normalizado est√° en la lista, o si consiste SOLO en s√≠mbolos como * o #...\n",
    "        if nombre_norm in placeholders_ruido or re.match(r'^[\\*#]+$', nombre_norm):\n",
    "            return None # ... lo consideramos inv√°lido.\n",
    "\n",
    "        # Si sobrevivi√≥ a todo y no est√° vac√≠o, ¬°lo devolvemos!\n",
    "        return nombre_norm if nombre_norm else None\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si algo falla en el proceso, avisamos y devolvemos el original.\n",
    "        log.warning(f\"Error normalizando el nombre '{nombre}': {e}\")\n",
    "        return nombre\n",
    "\n",
    "def identificar_y_reportar_duplicados(df_dim: pd.DataFrame, ruta_reporte: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Esta funci√≥n es nuestro detective de duplicados.\n",
    "    1. A√±ade la columna 'nombre_normalizado' al DataFrame.\n",
    "    2. Busca grupos de nombres normalizados que tengan M√ÅS DE UN 'id_nombre' diferente.\n",
    "    3. Guarda un reporte en CSV con estos grupos sospechosos.\n",
    "    4. Devuelve el DataFrame con la columna 'nombre_normalizado' (la quitaremos despu√©s).\n",
    "\n",
    "    Args:\n",
    "        df_dim (pd.DataFrame): DataFrame de la dimensi√≥n de nombres (ya con limpieza b√°sica).\n",
    "        ruta_reporte (Path): D√≥nde guardar el reporte de duplicados.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El mismo DataFrame pero con la columna 'nombre_normalizado' a√±adida.\n",
    "    \"\"\"\n",
    "    log.info(\"Iniciando normalizaci√≥n y b√∫squeda de posibles duplicados...\")\n",
    "    col_nombre = 'nombre'; col_id = 'id_nombre'; col_norm = 'nombre_normalizado'\n",
    "    col_flag = 'permiso_temporal' # Para incluir en el reporte\n",
    "\n",
    "    # Verificaci√≥n r√°pida: ¬øTenemos las columnas necesarias?\n",
    "    if not all(c in df_dim.columns for c in [col_nombre, col_id]):\n",
    "        log.error(f\"¬°Faltan columnas '{col_nombre}' o '{col_id}'! No podemos buscar duplicados.\")\n",
    "        # A√±adimos la columna normalizada como vac√≠a para no romper el flujo\n",
    "        df_dim[col_norm] = None\n",
    "        return df_dim\n",
    "\n",
    "    # Aplicamos la normalizaci√≥n a toda la columna 'nombre'\n",
    "    log.info(f\"Aplicando funci√≥n de normalizaci√≥n a '{col_nombre}'...\")\n",
    "    df_dim[col_norm] = df_dim[col_nombre].apply(normalizar_nombre_restaurante)\n",
    "    nulos_norm = df_dim[col_norm].isnull().sum()\n",
    "    if nulos_norm > 0:\n",
    "        log.warning(f\"{nulos_norm} nombres resultaron nulos/vac√≠os tras normalizar (ser√°n ignorados en la detecci√≥n).\")\n",
    "\n",
    "    # ¬°La b√∫squeda! Agrupamos por nombre normalizado y filtramos los grupos\n",
    "    # donde haya m√°s de un id_nombre √∫nico. Usamos dropna para ignorar los nulos.\n",
    "    duplicados_agrupados = df_dim.dropna(subset=[col_norm, col_id])\\\n",
    "                               .groupby(col_norm)\\\n",
    "                               .filter(lambda grupo: grupo[col_id].nunique() > 1)\n",
    "\n",
    "    if duplicados_agrupados.empty:\n",
    "        log.info(\"¬°Buenas noticias! No se encontraron grupos con IDs potencialmente duplicados por nombre normalizado.\")\n",
    "        # Podr√≠amos borrar un reporte viejo aqu√≠ si quisi√©ramos.\n",
    "    else:\n",
    "        num_grupos = duplicados_agrupados[col_norm].nunique()\n",
    "        log.warning(f\"¬°Ojo! Encontramos {num_grupos} grupos de nombres que parecen ser el mismo restaurante pero tienen IDs diferentes.\")\n",
    "        # Preparamos el reporte para que sea f√°cil de revisar\n",
    "        columnas_reporte = [col_norm, col_nombre, col_id]\n",
    "        if col_flag in duplicados_agrupados.columns: # Incluir el flag si existe\n",
    "            columnas_reporte.append(col_flag)\n",
    "        reporte = duplicados_agrupados[columnas_reporte].sort_values(by=[col_norm, col_nombre])\n",
    "        try:\n",
    "            # Guardamos el reporte\n",
    "            reporte.to_csv(ruta_reporte, index=False, encoding='utf-8-sig')\n",
    "            log.info(f\"Reporte de duplicados potenciales guardado en: {ruta_reporte}\")\n",
    "        except Exception as e:\n",
    "            log.exception(f\"¬°Error! No se pudo guardar el reporte de duplicados: {e}\")\n",
    "\n",
    "    log.info(\"Normalizaci√≥n e identificaci√≥n de duplicados completada.\")\n",
    "    # Devolvemos el DataFrame original con la columna normalizada a√±adida temporalmente\n",
    "    return df_dim\n",
    "\n",
    "def guardar_dimension(dim_df: pd.DataFrame, ruta_dim: Path):\n",
    "    \"\"\"\n",
    "    Guarda nuestro DataFrame de dimensi√≥n final en formato CSV.\n",
    "    Se asegura de que la columna de flag exista y tenga el tipo correcto.\n",
    "    Asume que las columnas ya est√°n seleccionadas y ordenadas como queremos.\n",
    "\n",
    "    Args:\n",
    "        dim_df (pd.DataFrame): El DataFrame de dimensi√≥n listo para guardar.\n",
    "        ruta_dim (Path): La ruta completa del archivo CSV de salida.\n",
    "    \"\"\"\n",
    "    log.info(f\"Guardando dimensi√≥n final ({len(dim_df)} filas) en: {ruta_dim}\")\n",
    "    if dim_df is None or dim_df.empty:\n",
    "        log.warning(\"El DataFrame de dimensi√≥n est√° vac√≠o, no se guardar√°.\")\n",
    "        return\n",
    "\n",
    "    # Doble chequeo y correcci√≥n del tipo de 'permiso_temporal'\n",
    "    col_flag = 'permiso_temporal'\n",
    "    if col_flag not in dim_df.columns:\n",
    "        log.warning(f\"A√±adiendo columna '{col_flag}' faltante con default=False antes de guardar.\")\n",
    "        dim_df[col_flag] = False\n",
    "    try:\n",
    "        # Forzar a booleano nullable de Pandas para consistencia\n",
    "        dim_df[col_flag] = dim_df[col_flag].astype('boolean')\n",
    "    except Exception as e:\n",
    "        log.warning(f\"No se pudo convertir '{col_flag}' a booleano, guardando como objeto: {e}\")\n",
    "        # Como fallback, aseguramos que no haya NaNs problem√°ticos para CSV\n",
    "        dim_df[col_flag] = dim_df[col_flag].fillna(False)\n",
    "\n",
    "    try:\n",
    "        # Guardamos el archivo CSV. index=False es importante. utf-8-sig ayuda con Excel.\n",
    "        dim_df.to_csv(ruta_dim, index=False, encoding='utf-8-sig')\n",
    "        log.info(f\"¬°Dimensi√≥n guardada exitosamente!\")\n",
    "    except Exception as e:\n",
    "        # ¬°Si esto falla, es un problema!\n",
    "        log.exception(f\"¬°ERROR CR√çTICO! No se pudo guardar la dimensi√≥n en '{ruta_dim}': {e}\")\n",
    "\n",
    "\n",
    "# --- Funci√≥n Principal de Limpieza (Nuestro Orquestador) ---\n",
    "def main_limpieza_dim_nombres():\n",
    "    \"\"\"\n",
    "    Orquesta todo el proceso: Carga la dimensi√≥n, aplica limpieza b√°sica,\n",
    "    normaliza nombres, reporta duplicados, ordena y guarda la versi√≥n final.\n",
    "    \"\"\"\n",
    "    log.info(\"================================================\")\n",
    "    log.info(\"=== INICIO Limpieza Dimensi√≥n Nombres Restaurante ===\")\n",
    "    log.info(\"================================================\")\n",
    "\n",
    "    # 1. Cargar Dimensi√≥n Existente\n",
    "    # -------------------------------\n",
    "    log.info(f\"Paso 1: Cargando dimensi√≥n desde: {RUTA_DIM_NOMBRES}\")\n",
    "    if not RUTA_DIM_NOMBRES.is_file():\n",
    "        log.critical(f\"¬°Abortando! No se encontr√≥ el archivo de dimensi√≥n: {RUTA_DIM_NOMBRES}\")\n",
    "        sys.exit(1) # Salimos si no existe el archivo que queremos limpiar\n",
    "    try:\n",
    "        # Le decimos a Pandas que trate IDs y nombres como texto al cargar, y el flag como objeto por ahora\n",
    "        dtype_map = {'id_nombre': str, 'nombre': str, 'permiso_temporal': object}\n",
    "        # keep_default_na=False ayuda a leer valores booleanos que podr√≠an estar como texto 'True'/'False'\n",
    "        dim_nombres = pd.read_csv(RUTA_DIM_NOMBRES, dtype=dtype_map, keep_default_na=False)\n",
    "        log.info(f\"Dimensi√≥n cargada: {dim_nombres.shape[0]} filas, {dim_nombres.shape[1]} columnas.\")\n",
    "    except Exception as e:\n",
    "        log.exception(f\"¬°Error cr√≠tico al cargar la dimensi√≥n! {e}\")\n",
    "        sys.exit(1)\n",
    "    if dim_nombres.empty:\n",
    "        log.warning(\"La dimensi√≥n est√° vac√≠a. No hay nada que limpiar. Saliendo.\")\n",
    "        return # Terminamos si no hay datos\n",
    "\n",
    "    # 2. Limpieza B√°sica Previa (¬°M√°s Robusta!)\n",
    "    # -----------------------------------------\n",
    "    log.info(\"Paso 2: Realizando limpieza b√°sica inicial...\")\n",
    "    original_rows = len(dim_nombres) # Guardamos el n√∫mero inicial de filas\n",
    "\n",
    "    # 2a. Asegurar columna y tipo 'permiso_temporal'\n",
    "    col_flag = 'permiso_temporal'\n",
    "    if col_flag not in dim_nombres.columns:\n",
    "        dim_nombres[col_flag] = False # La creamos si no existe\n",
    "        log.warning(f\"Columna '{col_flag}' no exist√≠a, se a√±adi√≥ con valor False.\")\n",
    "    # Convertimos a booleano de forma segura, manejando varios formatos de texto/n√∫mero\n",
    "    map_bool = {'true': True, 'false': False, '1': True, '0': False, '1.0': True, '0.0': False,\n",
    "                True: True, False: False, np.nan: False, None: False, '': False, 'nan': False}\n",
    "    dim_nombres[col_flag] = dim_nombres[col_flag].astype(str).str.lower().map(map_bool).fillna(False).astype('boolean')\n",
    "    log.info(f\"Columna '{col_flag}' asegurada como tipo Boolean (soporta nulos si los hubiera).\")\n",
    "\n",
    "    # 2b. Eliminar filas con IDs o Nombres inv√°lidos/vac√≠os/ruidosos ANTES de quitar duplicados\n",
    "    log.info(\"Filtrando filas con IDs o nombres inv√°lidos/vac√≠os/ruidosos...\")\n",
    "    # Quitar nulos expl√≠citos\n",
    "    dim_nombres.dropna(subset=['id_nombre', 'nombre'], inplace=True)\n",
    "    # Quitar los que quedaron como strings vac√≠os despu√©s de cargar/convertir\n",
    "    dim_nombres = dim_nombres[dim_nombres['nombre'].astype(str).str.strip() != '']\n",
    "    dim_nombres = dim_nombres[dim_nombres['id_nombre'].astype(str).str.strip() != '']\n",
    "    # Filtrar IDs que no sigan el formato NOM + 6 Hexadecimales\n",
    "    dim_nombres = dim_nombres[dim_nombres['id_nombre'].astype(str).str.match(r'^NOM[A-F0-9]{6}$')]\n",
    "    # Filtrar nombres que parezcan solo ruido (s√≠mbolos/placeholders)\n",
    "    placeholders_ruido = ['#NAME?', 'NAME', 'TEST']\n",
    "    dim_nombres = dim_nombres[~dim_nombres['nombre'].astype(str).str.match(r'^[#\\*$%\\?¬ø!¬°\\-_<>\",\\s]+$')] # Solo s√≠mbolos/espacios\n",
    "    dim_nombres = dim_nombres[~dim_nombres['nombre'].astype(str).str.upper().isin(placeholders_ruido)]\n",
    "    rows_dropped_invalid = original_rows - len(dim_nombres)\n",
    "    if rows_dropped_invalid > 0:\n",
    "        log.info(f\"Eliminadas {rows_dropped_invalid} filas con IDs/nombres inv√°lidos/vac√≠os/ruidosos.\")\n",
    "    original_rows = len(dim_nombres) # Actualizamos el contador para el siguiente paso\n",
    "\n",
    "    # 2c. Eliminar duplicados por nombre (ignorando may√∫sculas/min√∫sculas y espacios extra)\n",
    "    log.info(\"Eliminando duplicados por 'nombre' (insensible a may√∫s/min√∫s, sin espacios extra)...\")\n",
    "    # Creamos una columna temporal con el nombre limpio para comparar\n",
    "    dim_nombres['nombre_clean_temp'] = dim_nombres['nombre'].astype(str).str.strip().str.lower()\n",
    "    # Eliminamos duplicados basados en esta columna limpia, manteniendo la √öLTIMA aparici√≥n\n",
    "    # (consistente con c√≥mo los ETLs podr√≠an haber actualizado el flag permiso_temporal)\n",
    "    dim_nombres.drop_duplicates(subset=['nombre_clean_temp'], keep='last', inplace=True, ignore_index=True)\n",
    "    # Eliminamos la columna temporal que ya no necesitamos\n",
    "    dim_nombres.drop(columns=['nombre_clean_temp'], inplace=True)\n",
    "    rows_dropped_duplicates = original_rows - len(dim_nombres)\n",
    "    if rows_dropped_duplicates > 0:\n",
    "        log.info(f\"Eliminados {rows_dropped_duplicates} duplicados por 'nombre' (limpio, case-insensitive).\")\n",
    "\n",
    "    # Volvemos a verificar si nos quedamos sin datos despu√©s de limpiar\n",
    "    if dim_nombres.empty:\n",
    "        log.warning(\"La dimensi√≥n qued√≥ vac√≠a tras la limpieza b√°sica. No se puede continuar.\")\n",
    "        return\n",
    "\n",
    "    # 3. Normalizar Nombres y Reportar Duplicados Potenciales\n",
    "    # -------------------------------------------------------\n",
    "    # Esta funci√≥n A√ëADE la columna 'nombre_normalizado' temporalmente para el an√°lisis.\n",
    "    log.info(\"Paso 3: Normalizando nombres y generando reporte de duplicados...\")\n",
    "    dim_con_norm = identificar_y_reportar_duplicados(dim_nombres.copy(), RUTA_REPORTE_DUPLICADOS)\n",
    "    # Usamos una copia por si la funci√≥n modificara algo inesperadamente.\n",
    "\n",
    "    # 4. Ordenar Alfab√©ticamente\n",
    "    # ---------------------------\n",
    "    log.info(\"Paso 4: Ordenando la dimensi√≥n alfab√©ticamente por 'nombre'...\")\n",
    "    col_orden = 'nombre'\n",
    "    if col_orden in dim_con_norm.columns:\n",
    "        dim_con_norm.sort_values(\n",
    "            by=col_orden,\n",
    "            inplace=True,\n",
    "            ignore_index=True, # Resetea el √≠ndice final a 0, 1, 2...\n",
    "            na_position='last', # Por si alg√∫n nombre qued√≥ nulo (no deber√≠a pasar)\n",
    "            key=lambda col: col.astype(str).str.lower() # Orden case-insensitive\n",
    "        )\n",
    "        log.info(\"Dimensi√≥n ordenada.\")\n",
    "    else:\n",
    "        log.warning(f\"No se pudo ordenar, falta la columna '{col_orden}'.\")\n",
    "\n",
    "    # 5. Preparar para Guardar (¬°Quitando la columna normalizada!)\n",
    "    # -----------------------------------------------------------\n",
    "    log.info(\"Paso 5: Preparando DataFrame final para guardar...\")\n",
    "    col_a_quitar = 'nombre_normalizado'\n",
    "    if col_a_quitar in dim_con_norm.columns:\n",
    "        # Creamos el DataFrame final SIN la columna normalizada\n",
    "        dim_a_guardar = dim_con_norm.drop(columns=[col_a_quitar])\n",
    "        log.info(f\"Columna '{col_a_quitar}' eliminada para el archivo final.\")\n",
    "    else:\n",
    "        dim_a_guardar = dim_con_norm # Si no exist√≠a, usamos el DF tal cual\n",
    "        log.warning(f\"Columna '{col_a_quitar}' no encontrada para eliminar.\")\n",
    "\n",
    "    # Reordenamos las columnas que S√ç queremos guardar\n",
    "    cols_dim_final = ['id_nombre', 'nombre', 'permiso_temporal']\n",
    "    # Seleccionamos solo esas columnas en ese orden (si existen)\n",
    "    dim_final_guardar = dim_a_guardar[[col for col in cols_dim_final if col in dim_a_guardar.columns]]\n",
    "    log.info(f\"Columnas finales seleccionadas para guardar: {dim_final_guardar.columns.tolist()}\")\n",
    "\n",
    "    # 6. Guardar la Dimensi√≥n Limpia y Ordenada\n",
    "    # -----------------------------------------\n",
    "    log.info(\"Paso 6: Guardando la dimensi√≥n final...\")\n",
    "    # Llamamos a nuestra funci√≥n de guardado pas√°ndole el DF ya preparado\n",
    "    guardar_dimension(dim_final_guardar, RUTA_DIM_NOMBRES) # Sobreescribimos el archivo original\n",
    "\n",
    "    # --- Fin del Proceso ---\n",
    "    log.info(\"================================================\")\n",
    "    log.info(\"=== Limpieza Dimensi√≥n Nombres Completada ===\")\n",
    "    # Informamos si se gener√≥ el reporte de duplicados\n",
    "    if RUTA_REPORTE_DUPLICADOS.is_file(): # Verificamos si existe\n",
    "        log.info(f\"=== Reporte de posibles duplicados generado en: {RUTA_REPORTE_DUPLICADOS} ===\")\n",
    "        log.info(\"=== ¬°Revisa ese archivo para decidir c√≥mo unificar los IDs! ===\")\n",
    "    else:\n",
    "        log.info(\"=== No se gener√≥ reporte de duplicados (probablemente no se encontraron). ===\")\n",
    "    log.info(\"================================================\")\n",
    "\n",
    "\n",
    "# --- Punto de Entrada ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Esto asegura que main_limpieza_dim_nombres() solo se ejecute\n",
    "    # cuando corres este script directamente.\n",
    "    main_limpieza_dim_nombres()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
